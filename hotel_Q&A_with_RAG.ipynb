{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\"\n",
    "data_path = 'data/sorted_chat.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the document and split it into chunks\n",
    "loader = TextLoader(data_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma/'\n",
    "embedding_function = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector database and save it disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chuncks: 1905\n"
     ]
    }
   ],
   "source": [
    "# load it into Chroma and save to disk\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory=persist_directory)\n",
    "\n",
    "print(f\"Number of chuncks: {db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check RAG chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 03:14:44 Natalia None: –û—Ç–µ–ª—å –≤ —Å–æ—Å—Ç–∞–≤–µ –∏—Å–ø–∞–Ω—Å–∫–æ–π —Å–µ—Ç–∏, –æ–±–µ—Å–ø–µ—á–∏—Ç—å –ø–æ—Å—Ç–∞–≤–∫–∏ –π–æ–≥—É—Ä—Ç–æ–≤ –≤–ø–æ–ª–Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ, —Ö–∞–º–æ–Ω –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∞–≤–∏—Ç—å, –∞ –π–æ–≥—É—Ä—Ç—ã –Ω–µ—Ç, –Ω—É, —Å—Ç—Ä–∞–Ω–Ω–æ. –í Riu –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ç–µ—Ö –∂–µ –∏—Å–ø–∞–Ω—Ü–µ–≤ –µ—â–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç –Ω–∞–∑–∞–¥ –ø—Ä–æ–±–ª–µ–º –Ω–µ –±—ã–ª–æ. –ü–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ –∏–Ω—Ñ–æ–¥–µ–º–∏—è –ø–æ–¥–∫–æ—Å–∏–ª–∞ –º–Ω–æ–≥–∏—Ö, –Ω–æ –Ω–∞–¥–æ –Ω–∞–ª–∞–∂–∏–≤–∞—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –±–∞–∑–æ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã —Ö–æ—Ç—è –±—ã.\n",
      "2024-02-14 05:09:18 –ê—Ä—Ç—ë–º –ë None: –ê —Å–∫–æ–ª—å–∫–æ —É –≤–∞—Å –¥–µ—Ç–∏—à–µ–∫? –ò –∫–∞–∫ —á–∞—Å—Ç–æ –≤—ã –µ–∑–¥–∏—Ç–µ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É?\n",
      "#####################\n",
      "2024-04-09 16:15:33 Anna Bakshaeva: –†–µ–±—è—Ç, –∫—Ç–æ-—Ç–æ –µ–∑–¥–∏–ª –Ω–∞ —É—Ç—Ä–µ–Ω–Ω–∏–µ —ç–∫—Å–∫—É—Ä—Å–∏–∏, –±—Ä–∞–ª–∏ –∑–∞–≤—Ç—Ä–∞–∫ —Å —Å–æ–±–æ–π –∏–∑ –æ—Ç–µ–ª—è? –ï—Å—Ç—å –∑–¥–µ—Å—å –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö –æ—Ç–µ–ª—è—Ö –±–æ–∫—Å—ã?\n",
      "2024-04-09 18:21:13 Elena Kazakova: –ú—ã –µ–∑–¥–∏–ª–∏ –≤ –¢—Ä–∏–Ω–∏–¥–∞–¥ –≤ 6 —É—Ç—Ä–∞. –ë—Ä–∞–ª–∏ —Å –∑–∞–≤—Ç—Ä–∞–∫–∞ –π–æ–≥—É—Ä—Ç –≤ –±—É—Ç—ã–ª–æ—á–∫–∞—Ö –∏ –∫—Ä—É–∞—Å—Å–∞–Ω—ã, –∞ —Å —É–∂–∏–Ω–∞ –±—É—Ç–µ—Ä–±—Ä–æ–¥—ã —Å –∫–æ–ª–±–∞—Å–æ–π. –ö–æ—Ñ–µ –º–æ–∂–µ—Ç–µ –≤–∑—è—Ç—å —Å —Å–æ–±–æ–π –≤ –±–∞—Ä–µ. –ë–æ–∫—Å–æ–≤ –Ω–µ—Ç!\n",
      "#####################\n",
      "2024-03-27 22:46:10 –û–∫—Å–∞–Ω–∞ –í–ª–∞—Å–æ–≤–∞: –î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä.–ü–æ–¥–µ–ª–∏—Ç–µ—Å—å,–ø–æ–∂–∞–ª—É–π—Å—Ç–∞,–≥–¥–µ –ø–æ–∫—É–ø–∞–ª–∏ —Ä–æ–º?–°–∏–≥–∞—Ä—ã?\n",
      "2024-03-28 04:04:52 Viktory M: –°–∫–æ–ª—å–∫–æ –≤–∞—Å –±—ã–ª–æ —á–µ–ª–æ–≤–µ–∫ –∑–∞ —ç—Ç—É —Å—É–º–º—É?\n",
      "–í—Å–µ–º —Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è!\n",
      "–ë—É–¥—É –±–ª–∞–≥–æ–¥–∞—Ä–Ω–∞ –∑–∞ —Ñ–æ—Ç–æ üôåüèª‚òÄÔ∏è\n",
      "–ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –µ—Å—Ç—å –≤ —Å—Ç–æ–ª–æ–≤–æ–π –∫–∞–∫–∏–µ —Ç–æ –π–æ–≥—É—Ä—Ç—ã, –∫–µ—Ñ–∏—Ä? –ß–µ–º –º–æ–∂–Ω–æ –Ω–∞–∫–æ—Ä–º–∏—Ç—å —Ä–µ–±–µ–Ω–∫–∞ 1,5–≥\n",
      "2024-03-28 11:24:47 Levkina Evgeniya üåç ‚úàÔ∏è: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! \n",
      "2024-03-28 11:43:06 –ê–Ω–Ω–∞ None: —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ –π–æ–≥—É—Ä—Ç–æ–≤ –µ—Å—Ç—å, –≤–æ–ø—Ä–æ—Å –≤ –∏—Ö —Å—ä–µ–¥–æ–±–Ω–æ—Å—Ç–∏\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = '–ï—Å—Ç—å –ª–∏ –≤ –æ—Ç–µ–ª–µ –π–æ–≥—É—Ä—Ç?'\n",
    "docs = db.max_marginal_relevance_search(query, k=3, fetch_k=5)\n",
    "\n",
    "# print results\n",
    "for chunck in docs:\n",
    "    print(chunck.page_content)\n",
    "    print(\"#####################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vectordb from persist_directory saved above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=llm_name, temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, there are yogurts available in the hotel's restaurant. However, the guest mentioned that they are not from Spain, but rather some other type that they found to be chemically processed.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "You are a travel assistant for a major worldwide tourism company. You have access to customers chat of a given hotel. Here is the data: {context}\"\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = 'Are there yogurts?'\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are yogurts available in the hotel restaurant.\n"
     ]
    }
   ],
   "source": [
    "question = 'Are there yogurts?'\n",
    "result = qa.invoke({\"question\": question})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, it is unclear whether the yogurts are good or not. One person mentioned that there are yogurts available, but there is a question about their edibility.\n"
     ]
    }
   ],
   "source": [
    "question = \"Are they good?\"\n",
    "result = qa.invoke({\"question\": question})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, other options available for breakfast might include yogurt in bottles, croissants, sandwiches with sausage, and smoothies made from papaya and guava. Additionally, there might be some chemical-tasting yogurts, water, lemonades, and tea. It seems like there is a limited selection of non-alcoholic beverages and breakfast items available.\n"
     ]
    }
   ],
   "source": [
    "question = \"What else can I eat for breakfast?\"\n",
    "result = qa.invoke({\"question\": question})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_hotel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
